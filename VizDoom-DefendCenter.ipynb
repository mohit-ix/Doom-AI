{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e07cbcc6-21de-4627-9b5a-f554ee52a0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vizdoom import *\n",
    "import random\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac9e0550-c785-48d7-909b-408089268850",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = DoomGame()\n",
    "game.load_config('github/VizDoom/scenarios/defend_the_center.cfg')\n",
    "game.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f43418ee-11a7-44e8-808c-2e35daa9bd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.identity(3, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8af1cba-ee76-4cec-a867-7017374609c1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  100.0\n",
      "reward:  -1.0\n",
      "Result:  94.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "Result:  -390.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "Result:  -395.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  100.0\n",
      "reward:  -1.0\n",
      "Result:  95.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  100.0\n",
      "reward:  -1.0\n",
      "Result:  24.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  100.0\n",
      "reward:  -1.0\n",
      "Result:  92.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  100.0\n",
      "reward:  -1.0\n",
      "Result:  47.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  100.0\n",
      "reward:  -1.0\n",
      "Result:  -140.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m img \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mscreen_buffer\n\u001b[0;32m      7\u001b[0m info \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mgame_variables\n\u001b[1;32m----> 8\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreward: \u001b[39m\u001b[38;5;124m\"\u001b[39m, reward)\n\u001b[0;32m     10\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.02\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "for episode in range(episodes):\n",
    "    game.new_episode()\n",
    "    while not game.is_episode_finished():\n",
    "        state = game.get_state()\n",
    "        img = state.screen_buffer\n",
    "        info = state.game_variables\n",
    "        reward = game.make_action(random.choice(actions))\n",
    "        print(\"reward: \", reward)\n",
    "        time.sleep(0.02)\n",
    "    print(\"Result: \", game.get_total_reward())\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7316c970-ab78-425a-99b3-042fbca8fde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31db7ee4-34cd-44ee-b15d-0ff12d31808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cc4f141-1e93-471a-b545-540071361202",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VizDoomGym(Env): \n",
    "    # Function that is called when we start the env\n",
    "    def __init__(self, render=False): \n",
    "        # Inherit from Env\n",
    "        super().__init__()\n",
    "        # Setup the game \n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config('github/VizDoom/scenarios/defend_the_center.cfg')\n",
    "        \n",
    "        # Render frame logic\n",
    "        if render == False: \n",
    "            self.game.set_window_visible(False)\n",
    "        else:\n",
    "            self.game.set_window_visible(True)\n",
    "        \n",
    "        # Start the game \n",
    "        self.game.init()\n",
    "        \n",
    "        # Create the action space and observation space\n",
    "        self.observation_space = Box(low=0, high=255, shape=(100,160,1), dtype=np.uint8) \n",
    "        self.action_space = Discrete(3)\n",
    "        \n",
    "    # This is how we take a step in the environment\n",
    "    def step(self, action):\n",
    "        # Specify action and take step \n",
    "        actions = np.identity(3)\n",
    "        reward = self.game.make_action(actions[action], 4) \n",
    "        \n",
    "        # Get all the other stuff we need to retun \n",
    "        if self.game.get_state(): \n",
    "            state = self.game.get_state().screen_buffer\n",
    "            state = self.grayscale(state)\n",
    "            ammo = self.game.get_state().game_variables[0]\n",
    "            info = ammo\n",
    "        else: \n",
    "            state = np.zeros(self.observation_space.shape)\n",
    "            info = 0 \n",
    "        \n",
    "        info = {\"info\":info}\n",
    "        done = self.game.is_episode_finished()\n",
    "        \n",
    "        return state, reward, done, info \n",
    "    \n",
    "    # Define how to render the game or environment \n",
    "    def render(): \n",
    "        pass\n",
    "    \n",
    "    # What happens when we start a new game \n",
    "    def reset(self): \n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer\n",
    "        return self.grayscale(state)\n",
    "    \n",
    "    # Grayscale the game frame and resize it \n",
    "    def grayscale(self, observation):\n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "        resize = cv2.resize(gray, (160,100), interpolation=cv2.INTER_CUBIC)\n",
    "        state = np.reshape(resize, (100,160,1))\n",
    "        return state\n",
    "    \n",
    "    # Call to close down the game\n",
    "    def close(self): \n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3bda811-51b1-4cc1-be93-876569cce910",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "815d88b7-2d92-4201-8bb4-46aaf027cef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8b8bc30e-6268-4192-9353-604357d80bda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73e08b19-5022-4c4a-afaa-40aad751d9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common import env_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6597d28-6659-42fe-a8e8-eebb720b1cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_checker.check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7b3a23e-4c82-4c97-a889-4bfe6547ce9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "    \n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "        \n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "    \n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efd2ff7c-68e6-47a8-a1d7-d6a58b97911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/train_defend'\n",
    "LOG_DIR = './logs/log_defend'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36c3e721-1b90-4437-b068-5c95535bac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e65c85b5-7bbd-4266-89a3-22091416c1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f031190c-e2a8-4aa2-a25f-5d51140ecc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "212c6b56-2660-46d1-8d9a-a362eb2c9deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "model = PPO('CnnPolicy', env, verbose=1, learning_rate=0.0001, n_steps=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19f76a24-e2bb-4841-b804-41b3a6af007b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 78.6     |\n",
      "|    ep_rew_mean     | 0.231    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 147      |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 82.1         |\n",
      "|    ep_rew_mean          | 0.455        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 27           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 295          |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065618916 |\n",
      "|    clip_fraction        | 0.0299       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | -0.0215      |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.0244       |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00493     |\n",
      "|    value_loss           | 0.107        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90.3        |\n",
      "|    ep_rew_mean          | 1.14        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 445         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009634159 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.429       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00655     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    value_loss           | 0.113       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 100         |\n",
      "|    ep_rew_mean          | 1.9         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 593         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012760902 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.467       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00682    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.033      |\n",
      "|    value_loss           | 0.131       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 108         |\n",
      "|    ep_rew_mean          | 2.6         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 740         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016708182 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.986      |\n",
      "|    explained_variance   | 0.531       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0321     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0376     |\n",
      "|    value_loss           | 0.143       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 107        |\n",
      "|    ep_rew_mean          | 2.84       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 27         |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 881        |\n",
      "|    total_timesteps      | 24576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01894658 |\n",
      "|    clip_fraction        | 0.202      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.985     |\n",
      "|    explained_variance   | 0.601      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0296    |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.0448    |\n",
      "|    value_loss           | 0.138      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 117         |\n",
      "|    ep_rew_mean          | 3.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 1020        |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022297686 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.934      |\n",
      "|    explained_variance   | 0.687       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0057      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0432     |\n",
      "|    value_loss           | 0.144       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 127         |\n",
      "|    ep_rew_mean          | 4.65        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 1162        |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021352481 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.872      |\n",
      "|    explained_variance   | 0.665       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0337     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0447     |\n",
      "|    value_loss           | 0.176       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 139        |\n",
      "|    ep_rew_mean          | 5.49       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 28         |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 1309       |\n",
      "|    total_timesteps      | 36864      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02493368 |\n",
      "|    clip_fraction        | 0.205      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.843     |\n",
      "|    explained_variance   | 0.661      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0182    |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0466    |\n",
      "|    value_loss           | 0.193      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 146         |\n",
      "|    ep_rew_mean          | 6.2         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 1453        |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023540335 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.838      |\n",
      "|    explained_variance   | 0.763       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0387     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0469     |\n",
      "|    value_loss           | 0.17        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 151         |\n",
      "|    ep_rew_mean          | 6.59        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 1597        |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026460484 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.816      |\n",
      "|    explained_variance   | 0.765       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00847     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0445     |\n",
      "|    value_loss           | 0.192       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 152         |\n",
      "|    ep_rew_mean          | 6.83        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 1738        |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024502568 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.84       |\n",
      "|    explained_variance   | 0.842       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0763     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0481     |\n",
      "|    value_loss           | 0.15        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 154         |\n",
      "|    ep_rew_mean          | 7.14        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 1881        |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025774844 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.786      |\n",
      "|    explained_variance   | 0.826       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0315     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.047      |\n",
      "|    value_loss           | 0.18        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 153         |\n",
      "|    ep_rew_mean          | 7.26        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 2025        |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025721628 |\n",
      "|    clip_fraction        | 0.249       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.779      |\n",
      "|    explained_variance   | 0.844       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00991     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0469     |\n",
      "|    value_loss           | 0.188       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 151         |\n",
      "|    ep_rew_mean          | 7.28        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 2171        |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030026108 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.762      |\n",
      "|    explained_variance   | 0.843       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0278     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0472     |\n",
      "|    value_loss           | 0.166       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 150         |\n",
      "|    ep_rew_mean          | 7.23        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 2318        |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027790453 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.754      |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.000683    |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0464     |\n",
      "|    value_loss           | 0.148       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 150        |\n",
      "|    ep_rew_mean          | 7.15       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 28         |\n",
      "|    iterations           | 17         |\n",
      "|    time_elapsed         | 2465       |\n",
      "|    total_timesteps      | 69632      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03328149 |\n",
      "|    clip_fraction        | 0.265      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.724     |\n",
      "|    explained_variance   | 0.889      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0262    |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0476    |\n",
      "|    value_loss           | 0.173      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 154         |\n",
      "|    ep_rew_mean          | 7.52        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 2607        |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031292133 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.705      |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0225      |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0417     |\n",
      "|    value_loss           | 0.19        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 154         |\n",
      "|    ep_rew_mean          | 7.59        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 2758        |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030043285 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.692      |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0198     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0418     |\n",
      "|    value_loss           | 0.164       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 156         |\n",
      "|    ep_rew_mean          | 7.97        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 2908        |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028089926 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.695      |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0366     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0422     |\n",
      "|    value_loss           | 0.14        |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 160       |\n",
      "|    ep_rew_mean          | 8.44      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 28        |\n",
      "|    iterations           | 21        |\n",
      "|    time_elapsed         | 3055      |\n",
      "|    total_timesteps      | 86016     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0325396 |\n",
      "|    clip_fraction        | 0.237     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.657    |\n",
      "|    explained_variance   | 0.909     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.035    |\n",
      "|    n_updates            | 200       |\n",
      "|    policy_gradient_loss | -0.0428   |\n",
      "|    value_loss           | 0.142     |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 166         |\n",
      "|    ep_rew_mean          | 8.94        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 3204        |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032706194 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.618      |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0365     |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0432     |\n",
      "|    value_loss           | 0.14        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 174        |\n",
      "|    ep_rew_mean          | 9.45       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 28         |\n",
      "|    iterations           | 23         |\n",
      "|    time_elapsed         | 3352       |\n",
      "|    total_timesteps      | 94208      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03610131 |\n",
      "|    clip_fraction        | 0.248      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.568     |\n",
      "|    explained_variance   | 0.923      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0273    |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | -0.0433    |\n",
      "|    value_loss           | 0.122      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 175         |\n",
      "|    ep_rew_mean          | 9.62        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 3496        |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039649047 |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.564      |\n",
      "|    explained_variance   | 0.934       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0321     |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0409     |\n",
      "|    value_loss           | 0.118       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 179         |\n",
      "|    ep_rew_mean          | 9.99        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 3645        |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034361057 |\n",
      "|    clip_fraction        | 0.233       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.543      |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00785    |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0412     |\n",
      "|    value_loss           | 0.147       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x25ef204b730>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=100000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57f574cf-3235-4fe8-af86-c515cb1b9fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e79a2234-8aa9-4fb8-b3e9-73a9acf584b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = PPO.load('./train/train_defend/best_model_100000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2ca2943-b40c-4ecf-a31e-c14ccedf13d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b11d4b6a-5c21-4559-a96c-82202b4b8a6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\AIProjects\\Doom\\doom\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mean_reward, _ = evaluate_policy(new_model, env, n_eval_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "496dd9d9-52ee-47d4-b3a2-dfa69c2381fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward for episode 0 is 8.0\n",
      "Total reward for episode 1 is 15.0\n",
      "Total reward for episode 2 is 11.0\n",
      "Total reward for episode 3 is 13.0\n",
      "Total reward for episode 4 is 6.0\n"
     ]
    }
   ],
   "source": [
    "for episode in range(5):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action, _ = new_model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        time.sleep(0.05)\n",
    "        total_reward += reward\n",
    "    print('Total reward for episode {} is {}'.format(episode, total_reward))\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1faa825-9278-4eb8-8d64-8789c9670f61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doom",
   "language": "python",
   "name": "doom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e07cbcc6-21de-4627-9b5a-f554ee52a0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vizdoom import *\n",
    "import random\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac9e0550-c785-48d7-909b-408089268850",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = DoomGame()\n",
    "game.load_config('github/VizDoom/scenarios/basic.cfg')\n",
    "game.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f43418ee-11a7-44e8-808c-2e35daa9bd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.identity(3, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8af1cba-ee76-4cec-a867-7017374609c1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  100.0\n",
      "reward:  -1.0\n",
      "Result:  94.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "Result:  -390.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "Result:  -395.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  100.0\n",
      "reward:  -1.0\n",
      "Result:  95.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  100.0\n",
      "reward:  -1.0\n",
      "Result:  24.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  100.0\n",
      "reward:  -1.0\n",
      "Result:  92.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  100.0\n",
      "reward:  -1.0\n",
      "Result:  47.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  100.0\n",
      "reward:  -1.0\n",
      "Result:  -140.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n",
      "reward:  -6.0\n",
      "reward:  -1.0\n",
      "reward:  -1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m img \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mscreen_buffer\n\u001b[0;32m      7\u001b[0m info \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mgame_variables\n\u001b[1;32m----> 8\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreward: \u001b[39m\u001b[38;5;124m\"\u001b[39m, reward)\n\u001b[0;32m     10\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.02\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "for episode in range(episodes):\n",
    "    game.new_episode()\n",
    "    while not game.is_episode_finished():\n",
    "        state = game.get_state()\n",
    "        img = state.screen_buffer\n",
    "        info = state.game_variables\n",
    "        reward = game.make_action(random.choice(actions))\n",
    "        print(\"reward: \", reward)\n",
    "        time.sleep(0.02)\n",
    "    print(\"Result: \", game.get_total_reward())\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7316c970-ab78-425a-99b3-042fbca8fde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31db7ee4-34cd-44ee-b15d-0ff12d31808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cc4f141-1e93-471a-b545-540071361202",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VizDoomGym(Env): \n",
    "    # Function that is called when we start the env\n",
    "    def __init__(self, render=False): \n",
    "        # Inherit from Env\n",
    "        super().__init__()\n",
    "        # Setup the game \n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config('github/VizDoom/scenarios/basic.cfg')\n",
    "        \n",
    "        # Render frame logic\n",
    "        if render == False: \n",
    "            self.game.set_window_visible(False)\n",
    "        else:\n",
    "            self.game.set_window_visible(True)\n",
    "        \n",
    "        # Start the game \n",
    "        self.game.init()\n",
    "        \n",
    "        # Create the action space and observation space\n",
    "        self.observation_space = Box(low=0, high=255, shape=(100,160,1), dtype=np.uint8) \n",
    "        self.action_space = Discrete(3)\n",
    "        \n",
    "    # This is how we take a step in the environment\n",
    "    def step(self, action):\n",
    "        # Specify action and take step \n",
    "        actions = np.identity(3)\n",
    "        reward = self.game.make_action(actions[action], 4) \n",
    "        \n",
    "        # Get all the other stuff we need to retun \n",
    "        if self.game.get_state(): \n",
    "            state = self.game.get_state().screen_buffer\n",
    "            state = self.grayscale(state)\n",
    "            ammo = self.game.get_state().game_variables[0]\n",
    "            info = ammo\n",
    "        else: \n",
    "            state = np.zeros(self.observation_space.shape)\n",
    "            info = 0 \n",
    "        \n",
    "        info = {\"info\":info}\n",
    "        done = self.game.is_episode_finished()\n",
    "        \n",
    "        return state, reward, done, info \n",
    "    \n",
    "    # Define how to render the game or environment \n",
    "    def render(): \n",
    "        pass\n",
    "    \n",
    "    # What happens when we start a new game \n",
    "    def reset(self): \n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer\n",
    "        return self.grayscale(state)\n",
    "    \n",
    "    # Grayscale the game frame and resize it \n",
    "    def grayscale(self, observation):\n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "        resize = cv2.resize(gray, (160,100), interpolation=cv2.INTER_CUBIC)\n",
    "        state = np.reshape(resize, (100,160,1))\n",
    "        return state\n",
    "    \n",
    "    # Call to close down the game\n",
    "    def close(self): \n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b3bda811-51b1-4cc1-be93-876569cce910",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "815d88b7-2d92-4201-8bb4-46aaf027cef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8b8bc30e-6268-4192-9353-604357d80bda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73e08b19-5022-4c4a-afaa-40aad751d9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common import env_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b6597d28-6659-42fe-a8e8-eebb720b1cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_checker.check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e7b3a23e-4c82-4c97-a889-4bfe6547ce9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "    \n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "        \n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "    \n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "efd2ff7c-68e6-47a8-a1d7-d6a58b97911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/train_basic'\n",
    "LOG_DIR = './logs/log_basic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "36c3e721-1b90-4437-b068-5c95535bac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e65c85b5-7bbd-4266-89a3-22091416c1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f031190c-e2a8-4aa2-a25f-5d51140ecc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "212c6b56-2660-46d1-8d9a-a362eb2c9deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "model = PPO('CnnPolicy', env, verbose=1, learning_rate=0.0001, n_steps=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "19f76a24-e2bb-4841-b804-41b3a6af007b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 35.3     |\n",
      "|    ep_rew_mean     | -97.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 26       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 78       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 23.3         |\n",
      "|    ep_rew_mean          | -26.8        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 157          |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062449095 |\n",
      "|    clip_fraction        | 0.144        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.000143     |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 1.07e+03     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | 0.00128      |\n",
      "|    value_loss           | 2.4e+03      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 27.2        |\n",
      "|    ep_rew_mean          | -51.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 235         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014373511 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.112       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.29e+03    |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00731    |\n",
      "|    value_loss           | 3.49e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 27.4        |\n",
      "|    ep_rew_mean          | -48.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 313         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008987915 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.376       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.08e+03    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | 0.00159     |\n",
      "|    value_loss           | 2.63e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 21.4        |\n",
      "|    ep_rew_mean          | -17.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 393         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028753202 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.44        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.21e+03    |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.000563   |\n",
      "|    value_loss           | 3.15e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 23.6        |\n",
      "|    ep_rew_mean          | -26.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 472         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016946927 |\n",
      "|    clip_fraction        | 0.301       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.577       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.61e+03    |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | 0.00588     |\n",
      "|    value_loss           | 2.99e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29.8        |\n",
      "|    ep_rew_mean          | -64.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 646         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019872822 |\n",
      "|    clip_fraction        | 0.314       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.417       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.69e+03    |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | 0.0017      |\n",
      "|    value_loss           | 3.85e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 22.1        |\n",
      "|    ep_rew_mean          | -16.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 19          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 843         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020664653 |\n",
      "|    clip_fraction        | 0.309       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.414       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2.31e+03    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | 0.00446     |\n",
      "|    value_loss           | 3.53e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 14.9        |\n",
      "|    ep_rew_mean          | 22.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 17          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 1038        |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014605386 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | 0.525       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.8e+03     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | 0.00317     |\n",
      "|    value_loss           | 3.83e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 11.4        |\n",
      "|    ep_rew_mean          | 43.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 16          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 1241        |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024364054 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.966      |\n",
      "|    explained_variance   | 0.529       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.91e+03    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | 0.00429     |\n",
      "|    value_loss           | 4.35e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.2        |\n",
      "|    ep_rew_mean          | 24.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 15          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 1440        |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026129385 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.959      |\n",
      "|    explained_variance   | 0.421       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2.02e+03    |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | 0.035       |\n",
      "|    value_loss           | 3.37e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 12.4        |\n",
      "|    ep_rew_mean          | 38.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 15          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 1587        |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015930537 |\n",
      "|    clip_fraction        | 0.301       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.956      |\n",
      "|    explained_variance   | 0.633       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.34e+03    |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00418    |\n",
      "|    value_loss           | 2.98e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 9.37       |\n",
      "|    ep_rew_mean          | 57.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 16         |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 1658       |\n",
      "|    total_timesteps      | 26624      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03137787 |\n",
      "|    clip_fraction        | 0.304      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.931     |\n",
      "|    explained_variance   | 0.737      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 1.27e+03   |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.000336  |\n",
      "|    value_loss           | 2.4e+03    |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 8.41       |\n",
      "|    ep_rew_mean          | 60.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 16         |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 1742       |\n",
      "|    total_timesteps      | 28672      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06839256 |\n",
      "|    clip_fraction        | 0.399      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.891     |\n",
      "|    explained_variance   | 0.669      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 921        |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.00298   |\n",
      "|    value_loss           | 2.49e+03   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.64        |\n",
      "|    ep_rew_mean          | 71.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 16          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 1825        |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013291612 |\n",
      "|    clip_fraction        | 0.312       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.829      |\n",
      "|    explained_variance   | 0.789       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.11e+03    |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | 0.00295     |\n",
      "|    value_loss           | 2.05e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.74        |\n",
      "|    ep_rew_mean          | 67.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 17          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 1914        |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042554494 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.762      |\n",
      "|    explained_variance   | 0.761       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.02e+03    |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | 0.0323      |\n",
      "|    value_loss           | 1.35e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5.84       |\n",
      "|    ep_rew_mean          | 76.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 17         |\n",
      "|    iterations           | 17         |\n",
      "|    time_elapsed         | 2004       |\n",
      "|    total_timesteps      | 34816      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04670999 |\n",
      "|    clip_fraction        | 0.322      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.703     |\n",
      "|    explained_variance   | 0.511      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 261        |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | 0.0203     |\n",
      "|    value_loss           | 597        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.02        |\n",
      "|    ep_rew_mean          | 80.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 17          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 2097        |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041841354 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.577      |\n",
      "|    explained_variance   | 0.358       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 83.9        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | 0.00924     |\n",
      "|    value_loss           | 336         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.6         |\n",
      "|    ep_rew_mean          | 79.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 17          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 2190        |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027475085 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.492      |\n",
      "|    explained_variance   | 0.564       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 91.9        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | 0.000311    |\n",
      "|    value_loss           | 278         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.7         |\n",
      "|    ep_rew_mean          | 83.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 17          |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 2284        |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028315645 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.434      |\n",
      "|    explained_variance   | 0.611       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 49.7        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | 0.00364     |\n",
      "|    value_loss           | 135         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5.06       |\n",
      "|    ep_rew_mean          | 82.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 18         |\n",
      "|    iterations           | 21         |\n",
      "|    time_elapsed         | 2380       |\n",
      "|    total_timesteps      | 43008      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07045147 |\n",
      "|    clip_fraction        | 0.15       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.354     |\n",
      "|    explained_variance   | 0.716      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 53.9       |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | 0.0179     |\n",
      "|    value_loss           | 76.5       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 4.38       |\n",
      "|    ep_rew_mean          | 85.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 18         |\n",
      "|    iterations           | 22         |\n",
      "|    time_elapsed         | 2471       |\n",
      "|    total_timesteps      | 45056      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03255543 |\n",
      "|    clip_fraction        | 0.179      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.403     |\n",
      "|    explained_variance   | 0.546      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 55.6       |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | 0.0214     |\n",
      "|    value_loss           | 113        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.01        |\n",
      "|    ep_rew_mean          | 86.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 2567        |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039684303 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.335      |\n",
      "|    explained_variance   | 0.666       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 14.5        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | 0.0109      |\n",
      "|    value_loss           | 76.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.63        |\n",
      "|    ep_rew_mean          | 84.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 2666        |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024972223 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.262      |\n",
      "|    explained_variance   | 0.758       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 17.2        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | 0.031       |\n",
      "|    value_loss           | 42.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.26        |\n",
      "|    ep_rew_mean          | 85.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 2764        |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020400552 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.221      |\n",
      "|    explained_variance   | 0.81        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 8.79        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | 0.0183      |\n",
      "|    value_loss           | 28.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.05        |\n",
      "|    ep_rew_mean          | 86.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 2862        |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023111433 |\n",
      "|    clip_fraction        | 0.0842      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.186      |\n",
      "|    explained_variance   | 0.641       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 106         |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | 0.00829     |\n",
      "|    value_loss           | 85.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.41        |\n",
      "|    ep_rew_mean          | 84.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 2954        |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018603295 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.246      |\n",
      "|    explained_variance   | 0.686       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 62.9        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | 0.0159      |\n",
      "|    value_loss           | 73.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.88        |\n",
      "|    ep_rew_mean          | 87.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 3048        |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016217763 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.185      |\n",
      "|    explained_variance   | 0.545       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 26.8        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | 0.0108      |\n",
      "|    value_loss           | 53.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.72        |\n",
      "|    ep_rew_mean          | 87.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 3145        |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049227472 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.172      |\n",
      "|    explained_variance   | 0.683       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 15.4        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | 0.0159      |\n",
      "|    value_loss           | 50.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.22        |\n",
      "|    ep_rew_mean          | 89.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 3244        |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018328916 |\n",
      "|    clip_fraction        | 0.0751      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.181      |\n",
      "|    explained_variance   | 0.778       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 13.7        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | 0.0218      |\n",
      "|    value_loss           | 32.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4           |\n",
      "|    ep_rew_mean          | 86.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 18          |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 3342        |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011686733 |\n",
      "|    clip_fraction        | 0.0755      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.155      |\n",
      "|    explained_variance   | 0.776       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 11.4        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | 0.00585     |\n",
      "|    value_loss           | 33          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.65        |\n",
      "|    ep_rew_mean          | 88.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 19          |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 3444        |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010501701 |\n",
      "|    clip_fraction        | 0.064       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.152      |\n",
      "|    explained_variance   | 0.643       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 52.5        |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | 0.0129      |\n",
      "|    value_loss           | 75.7        |\n",
      "-----------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 4.02     |\n",
      "|    ep_rew_mean          | 86.6     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 19       |\n",
      "|    iterations           | 33       |\n",
      "|    time_elapsed         | 3543     |\n",
      "|    total_timesteps      | 67584    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.053387 |\n",
      "|    clip_fraction        | 0.069    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.144   |\n",
      "|    explained_variance   | 0.74     |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | 14.3     |\n",
      "|    n_updates            | 320      |\n",
      "|    policy_gradient_loss | 0.00836  |\n",
      "|    value_loss           | 42.9     |\n",
      "--------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.93        |\n",
      "|    ep_rew_mean          | 87          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 19          |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 3644        |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008264966 |\n",
      "|    clip_fraction        | 0.0594      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.125      |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 3.97        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | 0.0276      |\n",
      "|    value_loss           | 7.39        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.84        |\n",
      "|    ep_rew_mean          | 87.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 19          |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 3746        |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017383082 |\n",
      "|    clip_fraction        | 0.0729      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.123      |\n",
      "|    explained_variance   | 0.723       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 26.1        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | 0.0193      |\n",
      "|    value_loss           | 40.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.12        |\n",
      "|    ep_rew_mean          | 86.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 19          |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 3846        |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008167799 |\n",
      "|    clip_fraction        | 0.0558      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.109      |\n",
      "|    explained_variance   | 0.74        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 43.5        |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | 0.0157      |\n",
      "|    value_loss           | 43.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.08        |\n",
      "|    ep_rew_mean          | 86.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 19          |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 3944        |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033182986 |\n",
      "|    clip_fraction        | 0.0925      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.121      |\n",
      "|    explained_variance   | 0.713       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 25.5        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | 0.00887     |\n",
      "|    value_loss           | 53.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.59        |\n",
      "|    ep_rew_mean          | 88.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 19          |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 4042        |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023516357 |\n",
      "|    clip_fraction        | 0.0765      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.114      |\n",
      "|    explained_variance   | 0.769       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 12          |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | 0.00327     |\n",
      "|    value_loss           | 40.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.75        |\n",
      "|    ep_rew_mean          | 87.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 19          |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 4142        |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029292254 |\n",
      "|    clip_fraction        | 0.0679      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.116      |\n",
      "|    explained_variance   | 0.875       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 14.9        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | 0.0163      |\n",
      "|    value_loss           | 20.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.05        |\n",
      "|    ep_rew_mean          | 86          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 19          |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 4243        |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054000102 |\n",
      "|    clip_fraction        | 0.0935      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0981     |\n",
      "|    explained_variance   | 0.589       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 66.8        |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | 0.0169      |\n",
      "|    value_loss           | 79.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.06        |\n",
      "|    ep_rew_mean          | 85.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 19          |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 4341        |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022564815 |\n",
      "|    clip_fraction        | 0.0678      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0894     |\n",
      "|    explained_variance   | 0.745       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 10.4        |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | 0.00201     |\n",
      "|    value_loss           | 49          |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 3.92      |\n",
      "|    ep_rew_mean          | 87.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 19        |\n",
      "|    iterations           | 42        |\n",
      "|    time_elapsed         | 4444      |\n",
      "|    total_timesteps      | 86016     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1044795 |\n",
      "|    clip_fraction        | 0.0782    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.101    |\n",
      "|    explained_variance   | 0.631     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 29.2      |\n",
      "|    n_updates            | 410       |\n",
      "|    policy_gradient_loss | 0.00894   |\n",
      "|    value_loss           | 63.6      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 4.35      |\n",
      "|    ep_rew_mean          | 85.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 19        |\n",
      "|    iterations           | 43        |\n",
      "|    time_elapsed         | 4543      |\n",
      "|    total_timesteps      | 88064     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0206468 |\n",
      "|    clip_fraction        | 0.0602    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.125    |\n",
      "|    explained_variance   | 0.906     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 1.41      |\n",
      "|    n_updates            | 420       |\n",
      "|    policy_gradient_loss | 0.0384    |\n",
      "|    value_loss           | 10.9      |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.35        |\n",
      "|    ep_rew_mean          | 85.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 19          |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 4637        |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042179722 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.151      |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 7.66        |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.00131    |\n",
      "|    value_loss           | 12.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.9         |\n",
      "|    ep_rew_mean          | 87          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 19          |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 4738        |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031440847 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.152      |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 10.8        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | 0.00057     |\n",
      "|    value_loss           | 18.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.98        |\n",
      "|    ep_rew_mean          | 86.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 19          |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 4840        |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027293378 |\n",
      "|    clip_fraction        | 0.0845      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.121      |\n",
      "|    explained_variance   | 0.647       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 20.8        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | 0.0122      |\n",
      "|    value_loss           | 68.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.16        |\n",
      "|    ep_rew_mean          | 86.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 19          |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 4942        |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013872977 |\n",
      "|    clip_fraction        | 0.0616      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0973     |\n",
      "|    explained_variance   | 0.802       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 8.12        |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | 0.00821     |\n",
      "|    value_loss           | 30.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.51        |\n",
      "|    ep_rew_mean          | 88.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 19          |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 5046        |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009939363 |\n",
      "|    clip_fraction        | 0.0793      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.12       |\n",
      "|    explained_variance   | 0.831       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 6.64        |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | 0.0149      |\n",
      "|    value_loss           | 25.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.83        |\n",
      "|    ep_rew_mean          | 87.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 19          |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 5147        |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016510054 |\n",
      "|    clip_fraction        | 0.0517      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0871     |\n",
      "|    explained_variance   | 0.791       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 13.2        |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | 0.00888     |\n",
      "|    value_loss           | 35.9        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1f988595210>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=100000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57f574cf-3235-4fe8-af86-c515cb1b9fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e79a2234-8aa9-4fb8-b3e9-73a9acf584b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = PPO.load('./train/train_basic/best_model_10000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2ca2943-b40c-4ecf-a31e-c14ccedf13d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b11d4b6a-5c21-4559-a96c-82202b4b8a6d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m mean_reward, _ \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\AIProjects\\Doom\\doom\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:89\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[1;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (episode_counts \u001b[38;5;241m<\u001b[39m episode_count_targets)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m     88\u001b[0m     actions, states \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(observations, state\u001b[38;5;241m=\u001b[39mstates, episode_start\u001b[38;5;241m=\u001b[39mepisode_starts, deterministic\u001b[38;5;241m=\u001b[39mdeterministic)\n\u001b[1;32m---> 89\u001b[0m     observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m     current_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards\n\u001b[0;32m     91\u001b[0m     current_lengths \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mD:\\AIProjects\\Doom\\doom\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:162\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\AIProjects\\Doom\\doom\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:54\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[1;32m---> 54\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx]:\n\u001b[0;32m     58\u001b[0m             \u001b[38;5;66;03m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[0;32m     59\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterminal_observation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m obs\n",
      "Cell \u001b[1;32mIn[46], line 27\u001b[0m, in \u001b[0;36mVizDoomGym.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Specify action and take step \u001b[39;00m\n\u001b[0;32m     26\u001b[0m     actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39midentity(\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# Get all the other stuff we need to retun \u001b[39;00m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mget_state(): \n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mean_reward, _ = evaluate_policy(new_model, env, n_eval_episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "496dd9d9-52ee-47d4-b3a2-dfa69c2381fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward for episode 0 is 95.0\n",
      "Total reward for episode 1 is 51.0\n",
      "Total reward for episode 2 is 95.0\n",
      "Total reward for episode 3 is 95.0\n",
      "Total reward for episode 4 is 95.0\n"
     ]
    }
   ],
   "source": [
    "for episode in range(5):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action, _ = new_model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        time.sleep(0.05)\n",
    "        total_reward += reward\n",
    "    print('Total reward for episode {} is {}'.format(episode, total_reward))\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1faa825-9278-4eb8-8d64-8789c9670f61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doom",
   "language": "python",
   "name": "doom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
